{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7xyeCDq5NGATOno2yVEuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lewiskhu/colabtools/blob/main/paper_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GPAjzD3yOD0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Step 1: Get all papers from Google Scholar (handles pagination)\n",
        "def get_papers_from_scholar(user_id, delay=2):\n",
        "    papers = []\n",
        "    start = 0\n",
        "    while True:\n",
        "        url = f\"https://scholar.google.com/citations?hl=en&user={user_id}&view_op=list_works&sortby=pubdate&cstart={start}&pagesize=100\"\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        rows = soup.select('.gsc_a_tr')\n",
        "        if not rows:\n",
        "            break\n",
        "        for row in rows:\n",
        "            title_element = row.select_one('.gsc_a_at')\n",
        "            title = title_element.text if title_element else ''\n",
        "\n",
        "            # The journal/conference is usually the second .gs_gray element\n",
        "            gs_grays = row.select('.gs_gray')\n",
        "            journal = gs_grays[1].text if len(gs_grays) > 1 else ''\n",
        "\n",
        "            papers.append({'title': title, 'journal': journal})\n",
        "        start += 100\n",
        "        time.sleep(delay)  # Be polite to Google Scholar\n",
        "    return papers\n",
        "\n",
        "# Step 2: Check journal index status using Clarivate MJL or a local CSV\n",
        "def is_indexed(journal_name, sci_journals, ssci_journals, ahci_journals):\n",
        "    name = journal_name.strip().lower()\n",
        "    if name in sci_journals:\n",
        "        return \"SCI\"\n",
        "    elif name in ssci_journals:\n",
        "        return \"SSCI\"\n",
        "    elif name in ahci_journals:\n",
        "        return \"AHCI\"\n",
        "    return \"Not Indexed\"\n",
        "\n",
        "# Utility to load journal lists (CSV with journal names, one per line)\n",
        "def load_journal_list(path):\n",
        "    journals = set()\n",
        "    try:\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                if row:\n",
        "                    journals.add(row[0].strip().lower())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Journal list file not found at {path}. Using empty list.\")\n",
        "        # Return an empty set if the file is not found, allowing the rest of the code to run\n",
        "        return set()\n",
        "    return journals\n",
        "\n",
        "def main():\n",
        "    user_id = \"N4K7_bkAAAAJ\"  # Replace with your Scholar user ID\n",
        "    sci_journals = load_journal_list(\"sci_journals.csv\")\n",
        "    ssci_journals = load_journal_list(\"ssci_journals.csv\")\n",
        "    ahci_journals = load_journal_list(\"ahci_journals.csv\")\n",
        "\n",
        "    papers = get_papers_from_scholar(user_id)\n",
        "    with open(\"papers_with_index.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Title\", \"Journal\", \"Index\"])\n",
        "        for paper in papers:\n",
        "            index_status = is_indexed(paper['journal'], sci_journals, ssci_journals, ahci_journals)\n",
        "            writer.writerow([paper['title'], paper['journal'], index_status])\n",
        "            print(f\"{paper['title']} | {paper['journal']} | {index_status}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "277c1484"
      },
      "source": [
        "# Create empty placeholder CSV files for the journal lists\n",
        "# Replace these with your actual journal lists\n",
        "with open('sci_journals.csv', 'w') as f:\n",
        "    pass\n",
        "with open('ssci_journals.csv', 'w') as f:\n",
        "    pass\n",
        "with open('ahci_journals.csv', 'w') as f:\n",
        "    pass\n",
        "\n",
        "print(\"Placeholder journal list files created.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}